\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\documentclass[handout]{beamer}
%\usepackage[dvips]{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\usepackage{verbatim}

%\usetheme{Warsaw}
\usecolortheme{orchid}
\title{Bayes Estimators \& Ridge Regression}
\subtitle{Readings ISLR 6 }
\institute{Merlise Clyde}
\author{STA 521  Duke University}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle
\begin{frame} \frametitle{Model}

\begin{itemize}
\item Assume that we have centered (as before) and rescaled $\X^o$  (original $\X$) so that

$$\X_j =  \frac{\X^o_j - \bar{\X}^o_j}{\sum_i (X^o_{ij} - \bar{\X}^o_j)^2}$$
\pause
\item Equivalent to using `r scale(X)` \pause

\item Model:

$$\Y = \one \beta_0 + \X \b + \eps$$
\pause
\item $\X^T\X = (n-1) \Cor(\X) $  (correlation matrix of $X$) \pause
\item Eigenvalue Decomposition $\X^T\X = \U \Lambda \U^T$ \pause
\item if smallest eigen value is 0, $\X$ has columns that are linearly dependent
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{How Good are  Various Estimators}
Quadratic loss for estimating  $\b$ using estimator $\a$
$$ L(\b, \a) =  ( \b - \a)^T(\b -\a)$$ \pause

\begin{itemize}
\item Consider our expected loss (before we see the data) of taking an
``action'' $\a$ \pause
\item Under OLS or the  Reference prior the Expected Mean Square Error  \pause
  \begin{eqnarray*}
\E_\Y[( \b - \bhat)^T(\b -\bhat) & = &\sigma^2
  \tr[(\X^T\X)^{-1}] \pause \\
 & = & \sigma^2 \sum_{j=1}^p \lambda_j^{-1}
  \end{eqnarray*}
\pause
\item If smallest $\lambda_j \to 0$ then MSE $\to \infty$
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Problems}
Estimates:
$$\bhat = (\X^T\X)^{-1} \X^T \Y$$  or with $g$-prior

$$\bhat = \frac{g}{1+g}(\X^T\X)^{-1} \X^T \Y$$ may be unstable


Solutions:
  \begin{itemize}
  \item remove redundant variables (model selection)  (AIC, BIC, other approches)  $2^p$ models combinatorial hard problem even with Stochastic Search
\item add constant to  $\X^T\X$:  $\tilde{\b} = (\X^T\X  + k \I)^{-1} \X^T \Y$ to stabilise eigenvalues -  alternative shrinkage estimator
\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Independent Prior}
  \begin{itemize}
  \item Reference prior $p(\beta_0, \phi) \propto \phi^{-1}$
  \item
  Prior Distribution on $$\b \mid \phi, \beta_0, k \sim \N(\zero_p, \frac{1}{ \phi
    k} \I_p)$$ \pause

\item log likelihood (integrated) for $\b$ plus prior
$$- \frac{\phi}{2}\left(\| \Y - \one \bar{\Y} - \X \b \|^2  + k \| \b \|^2    \right)$$
\item  Posterior mean


  $$\bv_n = (\X^T\X + k \I)^{-1}  \X^T\X \bhat$$ \pause
\item importance of standardizing \pause

\item Choice of $k$ in practice?
\item $k = 0$ OLS
\item $k = \infty$  estimates are $\zero$   (intercept only)
  \end{itemize}
\end{frame}

\begin{frame}  \frametitle{Alternative Motivation}
  \begin{itemize}
  \item If  $\bhat$ is unconstrained  expect high variance with nearly
    singular $\X$ \pause

\item Control how large coefficients may grow \pause
    $$\min_{\b} (\Y - \one \bar{\Y}  - \X \b)^T (\Y -\one \bar{Y}  - \X\b)$$
    subject to
    $$ \sum \beta_j^2 \le t$$ \pause
  \item Equivalent Quadratic Programming Problem
    $$\min_{\b} \| \Y^c - \X^c \b\|^2 + k \|\b\|^2$$ \pause
  \item ``penalized'' likelihood  \pause
  \item Ridge Regression
  \end{itemize}
\end{frame}

\begin{frame}\frametitle{Geometry}
\includegraphics[height=2in]{ridge-constraint}
\footnote{onlinecourses.science.pse.edu}
\end{frame}
\begin{frame}
  \frametitle{Longley Data:  library(MASS); data(longley)}

\vspace{-12pt}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/data-1} 

}



\end{knitrout}
\end{frame}

\begin{frame}[fragile]
  \frametitle{OLS}
\begin{small}
\begin{verbatim}
> longley.lm = lm(Employed ~ ., data=longley)
> summary(longley.lm)

Coefficients:
               Estimate Std. Error t value Pr(>|t|)
(Intercept)  -3.482e+03  8.904e+02  -3.911 0.003560 **
GNP.deflator  1.506e-02  8.492e-02   0.177 0.863141
GNP          -3.582e-02  3.349e-02  -1.070 0.312681
Unemployed   -2.020e-02  4.884e-03  -4.136 0.002535 **
Armed.Forces -1.033e-02  2.143e-03  -4.822 0.000944 ***
Population   -5.110e-02  2.261e-01  -0.226 0.826212
Year          1.829e+00  4.555e-01   4.016 0.003037 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3049 on 9 degrees of freedom
Multiple R-squared: 0.9955,	Adjusted R-squared: 0.9925
F-statistic: 330.3 on 6 and 9 DF,  p-value: 4.984e-10
\end{verbatim}
\end{small}
\end{frame}


\begin{frame}[fragile]  \frametitle{Ridge Regression}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# from library MASS}
\hlstd{longley.ridge} \hlkwb{=} \hlkwd{lm.ridge}\hlstd{(Employed} \hlopt{~} \hlstd{.,} \hlkwc{data}\hlstd{=longley,}
                         \hlkwc{lambda}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0.0001}\hlstd{))}
\hlcom{# lambda = k in notes}

\hlkwd{summary}\hlstd{(longley.ridge)}
\end{alltt}
\begin{verbatim}
##        Length Class  Mode   
## coef   6006   -none- numeric
## scales    6   -none- numeric
## Inter     1   -none- numeric
## lambda 1001   -none- numeric
## ym        1   -none- numeric
## xm        6   -none- numeric
## GCV    1001   -none- numeric
## kHKB      1   -none- numeric
## kLW       1   -none- numeric
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}


\begin{frame}[fragile]  \frametitle{Ridge Trace Plot}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/ridgetrace-1} 

\end{knitrout}
\end{frame}


\begin{frame}[fragile]\frametitle{Choice of $k$}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{k} \hlkwb{=} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0.0001}\hlstd{)}
\hlstd{n.k} \hlkwb{=} \hlkwd{length}\hlstd{(k); n} \hlkwb{=} \hlkwd{nrow}\hlstd{(longley)}
\hlstd{cv.lambda} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{, n, n.k)}

\hlstd{rmse.ridge} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{data}\hlstd{,} \hlkwc{i}\hlstd{,} \hlkwc{j}\hlstd{,} \hlkwc{k}\hlstd{) \{}
  \hlstd{m.ridge} \hlkwb{=} \hlkwd{lm.ridge}\hlstd{(Employed} \hlopt{~} \hlstd{.,} \hlkwc{data} \hlstd{= data,} \hlkwc{lambda}\hlstd{=k[j],}
                          \hlkwc{subset} \hlstd{=} \hlopt{-}\hlstd{i)}
  \hlstd{yhat} \hlkwb{=} \hlkwd{scale}\hlstd{(data[i,}\hlnum{1}\hlopt{:}\hlnum{6}\hlstd{,} \hlkwc{drop}\hlstd{=F],}\hlkwc{center} \hlstd{= m.ridge}\hlopt{$}\hlstd{xm,}
                       \hlkwc{scale} \hlstd{= m.ridge}\hlopt{$}\hlstd{scales)} \hlopt{%*%}
                \hlstd{m.ridge}\hlopt{$}\hlstd{coef} \hlopt{+} \hlstd{m.ridge}\hlopt{$}\hlstd{ym}
\hlstd{(yhat} \hlopt{-} \hlstd{data}\hlopt{$}\hlstd{Employed[i])}\hlopt{^}\hlnum{2}
\hlstd{\}}

\hlkwa{for} \hlstd{(i}  \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n)  \{}
  \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n.k) \{}
    \hlstd{cv.lambda[i,j]} \hlkwb{=} \hlkwd{rmse.ridge}\hlstd{(longley, i, j, k)}
  \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile] \frametitle{Cross Validation Error}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{cv.error} \hlkwb{=} \hlkwd{apply}\hlstd{(cv.lambda,} \hlnum{2}\hlstd{, mean)}
\hlkwd{plot}\hlstd{(k, cv.error,} \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-1-1} 

\end{knitrout}
Best $k$ = 0.0028

\end{frame}

\begin{frame}[fragile]
\frametitle{Generalized Cross-validation}
\begin{small}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{select}\hlstd{(}\hlkwd{lm.ridge}\hlstd{(Employed} \hlopt{~} \hlstd{.,} \hlkwc{data}\hlstd{=longley,}
         \hlkwc{lambda}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0.0001}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## modified HKB estimator is 0.004275357 
## modified L-W estimator is 0.03229531 
## smallest value of GCV  at 0.0028
\end{verbatim}
\begin{alltt}
\hlstd{best.k} \hlkwb{=} \hlstd{longley.ridge}\hlopt{$}\hlstd{lambda[}\hlkwd{which.min}\hlstd{(longley.ridge}\hlopt{$}\hlstd{GCV)]}
\hlstd{longley.RReg} \hlkwb{=} \hlkwd{lm.ridge}\hlstd{(Employed} \hlopt{~} \hlstd{.,} \hlkwc{data}\hlstd{=longley,}
                        \hlkwc{lambda}\hlstd{=best.k)}
\hlkwd{coef}\hlstd{(longley.RReg)}
\end{alltt}
\begin{verbatim}
##                GNP.deflator           GNP    Unemployed  Armed.Forces 
## -2.950348e+03 -5.381450e-04 -1.822639e-02 -1.761107e-02 -9.607256e-03 
##    Population          Year 
## -1.185103e-01  1.557856e+00
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{small}

\end{frame}








\begin{frame}
  \frametitle{Priors on $k$}

$\X$ is centered and standardized
  \begin{eqnarray*}
  \Y  & =  &\one \beta_0 + \X  \b + \eps
  \end{eqnarray*}

Hierarchical prior \pause
\begin{itemize}
\item $p(\beta_0,  \phi \mid \b, \kappa) \propto \phi^{-1}$ \pause
\item $\b \mid \phi, \kappa \sim \N(\zero, \I (\phi \kappa )^{-1})$ \pause
\item prior on $\kappa$?  \pause

\item Take $$\kappa \mid \phi \sim  \Gam(1/2, 1/2)$$ \pause

\item What is induced prior on $\b \mid \phi$?
\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Posterior Distributions}
Joint Distribution
  \begin{itemize}
 \item $\beta_0, \b, \phi \mid \kappa, \Y$  Normal-Gamma family given $\Y$
   and $\kappa$ \pause
  \item $\kappa \mid \Y$  not tractable \pause
  \end{itemize}
Obtain marginal for  $\b$ via  MCMC \pause
%\begin{itemize}

Pick initial values $\beta_0^{(0)}, \b^{(0)},
  \phi^{(0)}$, \pause \\

Set  $t = 1$

\begin{enumerate}
 \item Sample $\kappa^{(t)} \sim p(\kappa \mid \beta_0^{(t-1)},
    \b^{(t-1)}, \phi^{(t-1)}, \Y)$ \pause
\item Sample $\beta_0^{(t)}, \b^{(t)}, \phi^{(t)} \mid \kappa(t),
     \Y$ \pause
\item  Set $t = t + 1$ and repeat until $t > T$ \pause
\end{enumerate}
Use Samples  $\beta_0^{(t)}, \b^{(t)}, \phi^{(t)}, \kappa^{(t)}$ for $t
= B, \ldots, T$ for inference

\end{frame}

\begin{frame} \frametitle{JAGS}
JAGS = Just Another Gibbs Sampler \pause

\begin{itemize}
\item scripting language to express sampling models and priors \pause
\item "derives" full conditional distributions \pause
\item integrates with R \pause
\item typically faster than interpreted R code \pause
\item accounts for uncertainty about $k$ \pause
\end{itemize}

How would you compare Bayes predictions with Ridge with Cross-validation?
\end{frame}
\end{document}

