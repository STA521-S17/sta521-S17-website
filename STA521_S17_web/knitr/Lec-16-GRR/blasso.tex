%\documentclass{beamer}
\documentclass[handout]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\usepackage[dvips]{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\usepackage{verbatim}

%\usetheme{Warsaw}
\usecolortheme{orchid}
\title{Generalized Ridge \& Lasso Regression}
\subtitle{Readings ISLR 6,  Casella \& Park }
\institute{Merlise Clyde}
\author{STA 521  Duke University}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle


\begin{frame} \frametitle{Model}

\begin{itemize}
\item Model: $\Y = \one \beta_0 + \X \b + \eps$  $\X$ is centered and scaled predictors
\pause
\item (Classical) Ridge Regression controls
 how large coefficients may grow \pause
    $$\min_{\b} (\Y - \one \bar{\Y}  - \X \b)^T (\Y -\one \bar{Y}  - \X\b)$$
    subject to
    $$ \sum \beta_j^2 \le t$$ \pause
  \item Penalized Likelihood
    $$\min_{\b} \| \Y - \one  \bar{\Y}  - \X \b\|^2 + k \|\b\|^2$$ \pause
\item  Bayesian Ridge Regression - Hierarchical prior \pause
\begin{itemize}
\item $p(\beta_0,  \phi \mid \b, \kappa) \propto \phi^{-1}$ \pause
\item $\b \mid \phi, \kappa \sim \N(\zero, \I (\phi \kappa )^{-1})$ \pause
\item prior on $\kappa$
\end{itemize}
\end{itemize}
For fixed $\kappa$ the Bayes MAP and the penalized MLE are the same
\end{frame}



\begin{frame} \frametitle{Differences}

Treatement of uncertainty  \pause
\begin{itemize}
\item Frequentist:  use of cross validation or optimization for finding  k  \pause
\item Bayes: removes "nuisance" parameter $\kappa$ through integration rather than than optimization  \pause
\begin{itemize}
\item  Can use full posterior distribution for credible intervals for parameters, regression function or predictions
 \pause
\item Other Choices of priors?
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Lasso}
Tibshirani (JRSS B 1996) proposed estimating coefficients through
$L_1$ constrained least squares  ``Least Absolute Shrinkage and
Selection Operator''  \pause
\begin{itemize}
 \item Control how large coefficients may grow \pause
    $$\min_{\b} (\Y - \one \bar{Y} - \X \b)^T (\Y -  \one \bar{Y} - \X\b)$$
    subject to
    $$ \sum |\beta_j| \le t$$ \pause
  \item Equivalent Quadratic Programming Problem for ``penalized'' Likelihood
    $$\min_{\b} \| \Y -\one\bar{Y} - \X \b\|^2 + \lambda \|\b\|_1$$ \pause
  \item Posterior mode
  $$
\max_{\b} - \{ \| \Y -\one \beta_0 - \X \b\|^2 + \lambda \|\b\|_1 \}
$$
\end{itemize}
\end{frame}

\begin{frame}  \frametitle{Variable Selection via the LASSO }
$p = 2$  constraint $|\beta_1| + |\beta_2| \le t$ is a diamond

\vfill


\end{frame}


\begin{frame}\frametitle[fragile]{R Code}
Path of solutions can be found using the
   ``Least Angle Regression'' Algorithm of Efron et al (Annals of
   Statistics 2004)   \pause



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(lars)}
\hlstd{longley.lars} \hlkwb{=} \hlkwd{lars}\hlstd{(}\hlkwd{as.matrix}\hlstd{(longley[,}\hlopt{-}\hlnum{7}\hlstd{]), longley[,}\hlnum{7}\hlstd{],}
                 \hlkwc{type}\hlstd{=}\hlstr{"lasso"}\hlstd{)}
\hlkwd{plot}\hlstd{(longley.lars)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/lars-1} 

}



\end{knitrout}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Solutions}
{\fontsize{9pt}{10pt} \selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{kable}\hlstd{(}\hlkwd{coef}\hlstd{(longley.lars),} \hlkwc{digits}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\begin{tabular}{r|r|r|r|r|r}
\hline
GNP.deflator & GNP & Unemployed & Armed.Forces & Population & Year\\
\hline
0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000\\
\hline
0.0000 & 0.0327 & 0.0000 & 0.0000 & 0.0000 & 0.0000\\
\hline
0.0000 & 0.0362 & -0.0037 & 0.0000 & 0.0000 & 0.0000\\
\hline
0.0000 & 0.0372 & -0.0046 & -0.0010 & 0.0000 & 0.0000\\
\hline
0.0000 & 0.0000 & -0.0124 & -0.0054 & 0.0000 & 0.9068\\
\hline
0.0000 & 0.0000 & -0.0141 & -0.0071 & 0.0000 & 0.9438\\
\hline
0.0000 & 0.0000 & -0.0147 & -0.0086 & -0.1534 & 1.1843\\
\hline
-0.0077 & 0.0000 & -0.0148 & -0.0087 & -0.1708 & 1.2289\\
\hline
0.0000 & -0.0121 & -0.0166 & -0.0093 & -0.1303 & 1.4319\\
\hline
0.0000 & -0.0253 & -0.0187 & -0.0099 & -0.0951 & 1.6865\\
\hline
0.0151 & -0.0358 & -0.0202 & -0.0103 & -0.0511 & 1.8292\\
\hline
\end{tabular}


\end{knitrout}
}

Which one?
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary}
\begin{small}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sum.lars} \hlkwb{=} \hlkwd{summary}\hlstd{(longley.lars)}
\hlstd{sum.lars}
\end{alltt}
\begin{verbatim}
## LARS/LASSO
## Call: lars(x = as.matrix(longley[, -7]), y = longley[, 7], type = "lasso")
##    Df     Rss        Cp
## 0   1 185.009 1976.7120
## 1   2   6.642   59.4712
## 2   3   3.883   31.7832
## 3   4   3.468   29.3165
## 4   5   1.563   10.8183
## 5   4   1.339    6.4068
## 6   5   1.024    5.0186
## 7   6   0.998    6.7388
## 8   7   0.907    7.7615
## 9   6   0.847    5.1128
## 10  7   0.836    7.0000
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{small}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Cp Solution}
Min $C_p = SSE_p/\hat{\sigma}^2_F - n + 2 p$  \pause

For a model that includes all true predictors $C_p \approx p$
\begin{small}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n.sol} \hlkwb{=} \hlkwd{length}\hlstd{(sum.lars}\hlopt{$}\hlstd{Cp)}
\hlstd{best} \hlkwb{=} \hlkwd{which.min}\hlstd{(}\hlkwd{abs}\hlstd{(sum.lars}\hlopt{$}\hlstd{Cp} \hlopt{-} \hlstd{sum.lars}\hlopt{$}\hlstd{Df)[}\hlopt{-}\hlstd{n.sol])}
\hlkwd{kable}\hlstd{(}\hlkwd{coef}\hlstd{(longley.lars)[best,],} \hlkwc{digits}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\begin{tabular}{l|r}
\hline
GNP.deflator & 0.0000\\
\hline
GNP & 0.0000\\
\hline
Unemployed & -0.0147\\
\hline
Armed.Forces & -0.0086\\
\hline
Population & -0.1534\\
\hline
Year & 1.1843\\
\hline
\end{tabular}


\end{knitrout}
\end{small}

 \pause
Can also use Cross-Validation - many packages available!

 \pause
What about uncertainty?  Confidence intervals?

\end{frame}

\begin{frame}
  \frametitle{Bayesian Lasso}
  Park \& Casella (JASA 2008) and Hans (Biometrika 2010) propose
  Bayesian versions of the Lasso  \pause
  \begin{eqnarray*}
    \Y \mid \beta_0, \b, \phi & \sim & \N(\one_n \beta_0 + \X \b, \I_n/\phi)  \pause\\
    \b \mid \beta_0, \phi, \taub & \sim & \N(\zero, \diag(\taub^2)/\phi)  \pause\\
    \tau_1^2 \ldots, \tau_p^2 \mid \beta_0, \phi & \simiid & \Ex(\lambda^2/2)  \pause\\
    p(\beta_0, \phi) & \propto& 1/\phi  \pause\\
  \end{eqnarray*}
Can show that $\beta_j \mid \phi, \lambda \simiid DE(\lambda \sqrt{\phi})$
$$\int_0^\infty \frac{1}{\sqrt{2 \pi s}}
  e^{-\frac{1}{2} \phi \frac{\beta^2}{s }}
  \, \frac{\lambda^2}{2} e^{- \frac{\lambda^2 s}{2}}\, ds =
  \frac{\lambda \phi^{1/2}}{2} e^{-\lambda \phi^{1/2} |\beta|}
$$  \pause
Scale Mixture of Normals  (Andrews and Mallows 1974)
\end{frame}

\begin{frame}\frametitle{Densities}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-1} 

}



\end{knitrout}

\end{frame}

\begin{frame}[fragile] \frametitle{Bayesian Lasso Fitting}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(diabetes)}
\hlstd{Y} \hlkwb{=} \hlstd{diabetes}\hlopt{$}\hlstd{y}
\hlstd{X} \hlkwb{=} \hlstd{diabetes}\hlopt{$}\hlstd{x2}  \hlcom{# 64 variables from all 10 main effects,}
                  \hlcom{# two-way interactions and quadradics}
\hlkwd{set.seed}\hlstd{(}\hlnum{8675309}\hlstd{)}
\hlkwd{suppressMessages}\hlstd{(}\hlkwd{library}\hlstd{(monomvn))}

\hlcom{## Ordinary Least Squares regression from monomvn}
\hlstd{reg.ols} \hlkwb{<-} \hlkwd{regress}\hlstd{(X, Y)}
\hlcom{## ridge regression}
\hlstd{reg.ridge} \hlkwb{<-} \hlkwd{regress}\hlstd{(X, Y,} \hlkwc{method}\hlstd{=}\hlstr{"ridge"}\hlstd{)}
\hlcom{## Lasso regression from monomvn}
\hlstd{reg.las} \hlkwb{<-} \hlkwd{regress}\hlstd{(X, Y,} \hlkwc{method}\hlstd{=}\hlstr{"lasso"}\hlstd{)}

\hlcom{## Bayesian Lasso regression from monomvn}
\hlstd{reg.blas} \hlkwb{<-} \hlkwd{blasso}\hlstd{(X, Y,} \hlkwc{RJ}\hlstd{=}\hlnum{FALSE}\hlstd{,} \hlkwc{verb}\hlstd{=}\hlnum{0}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]\frametitle{Estimates}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plots_blas-1} 

}



\end{knitrout}
\end{frame}


\begin{frame}  \frametitle{Shrinkage}
\vspace{-16pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/shrinkage-1} 

}



\end{knitrout}
\end{frame}

\begin{frame}\frametitle{Summary}

\begin{itemize}
\item Bayesian and Regular LASSO shrink (unstable) coefficients to zero  \pause
\item Bayesian posterior mean cannont be zero (so no selection)  \pause
\item Bayesian MAP (Maximum a posteriori) estimate equivalent to Lasso penalized MLE for same $\lambda$  \pause
\item Bayesian allows uncertainty in $\lambda$ to propogate to estimates and predictions  \pause
\item Bayesian MAP estimates via EM algorithms or Variational Bayes (STAN)  \pause
\item Report MAP estimate and HPD intervals
\pause
\item RJ = TRUE incorporates probabilty that $\beta = 0$ for variable selection

\end{itemize}

\end{frame}
\end{document}

